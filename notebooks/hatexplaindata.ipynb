{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Cell - Add this at the top of each notebook\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Set base directories based on environment\n",
    "if IS_KAGGLE:\n",
    "    INPUT_ROOT = \"/kaggle/input\"\n",
    "    WORK_DIR = \"/kaggle/working\"\n",
    "elif IS_COLAB:\n",
    "    INPUT_ROOT = \"/content/input\"\n",
    "    WORK_DIR = \"/content/working\"\n",
    "else:\n",
    "    # Local environment\n",
    "    INPUT_ROOT = Path.cwd() / \"input\"\n",
    "    WORK_DIR = Path.cwd() / \"working\"\n",
    "\n",
    "# Create standard directories\n",
    "OUT_DIR = os.path.join(WORK_DIR, \"data\")\n",
    "EXPERIMENTS_DIR = os.path.join(WORK_DIR, \"experiments\")\n",
    "SCRIPTS_DIR = os.path.join(WORK_DIR, \"scripts\")\n",
    "\n",
    "# Create all directories\n",
    "for directory in [OUT_DIR, EXPERIMENTS_DIR, SCRIPTS_DIR]:\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Environment: {'Kaggle' if IS_KAGGLE else 'Colab' if IS_COLAB else 'Local'}\")\n",
    "print(f\"Input directory: {INPUT_ROOT}\")\n",
    "print(f\"Working directory: {WORK_DIR}\")\n",
    "print(f\"Data directory: {OUT_DIR}\")\n",
    "print(f\"Experiments directory: {EXPERIMENTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2de0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HateXplain Dataset Preprocessing\n",
    "import os, json\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "INPUT_ROOT = \"/kaggle/input\"\n",
    "WORK_DIR   = \"/kaggle/working\"\n",
    "OUT_DIR    = os.path.join(WORK_DIR, \"data\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Locate HateXplain dataset\n",
    "HATEX_DIRS = [os.path.join(INPUT_ROOT, d) for d in os.listdir(INPUT_ROOT)\n",
    "              if \"hatexplain\" in d.lower() or \"hate_explain\" in d.lower()]\n",
    "\n",
    "if len(HATEX_DIRS) == 0:\n",
    "    print(\"[WARN] HateXplain dataset not found. Please add it to Kaggle inputs.\")\n",
    "else:\n",
    "    HATEX_DIR = HATEX_DIRS[0]\n",
    "    print(f\"Found HateXplain at: {HATEX_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b9649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load HateXplain data\n",
    "# HateXplain typically comes as JSON or JSONL\n",
    "import json\n",
    "\n",
    "def load_hatexplain_json(filepath):\n",
    "    \"\"\"Load HateXplain from JSON or JSONL format.\"\"\"\n",
    "    records = []\n",
    "    \n",
    "    # Try as JSONL first\n",
    "    try:\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    records.append(json.loads(line))\n",
    "    except:\n",
    "        # Try as single JSON\n",
    "        with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "            if isinstance(data, list):\n",
    "                records = data\n",
    "            elif isinstance(data, dict):\n",
    "                # Sometimes HateXplain is stored as {id: record}\n",
    "                records = list(data.values())\n",
    "    \n",
    "    return records\n",
    "\n",
    "# Find JSON/JSONL files\n",
    "json_files = [f for f in os.listdir(HATEX_DIR) if f.endswith((\".json\", \".jsonl\"))]\n",
    "if not json_files:\n",
    "    raise FileNotFoundError(f\"No JSON/JSONL files found in {HATEX_DIR}\")\n",
    "\n",
    "hatex_file = json_files[0]\n",
    "print(f\"Loading: {hatex_file}\")\n",
    "\n",
    "records = load_hatexplain_json(os.path.join(HATEX_DIR, hatex_file))\n",
    "print(f\"Loaded {len(records)} records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2842c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse HateXplain records\n",
    "rows = []\n",
    "\n",
    "for idx, rec in enumerate(records):\n",
    "    # Extract text (either as string or from tokens)\n",
    "    if isinstance(rec.get(\"text\"), str):\n",
    "        text = rec[\"text\"]\n",
    "    elif \"post_tokens\" in rec:\n",
    "        text = \" \".join(rec[\"post_tokens\"])\n",
    "    else:\n",
    "        continue\n",
    "    \n",
    "    # Extract label\n",
    "    # HateXplain labels: \"hatespeech\", \"offensive\", \"normal\"\n",
    "    label_str = str(rec.get(\"label\", \"normal\")).lower()\n",
    "    \n",
    "    # Map to binary: toxic (1) or non-toxic (0)\n",
    "    if label_str in {\"hatespeech\", \"offensive\", \"offensive_language\", \"hate\"}:\n",
    "        label = 1\n",
    "    else:\n",
    "        label = 0\n",
    "    \n",
    "    rows.append({\n",
    "        \"id\": rec.get(\"post_id\", idx),\n",
    "        \"text\": text,\n",
    "        \"label\": label\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(f\"Parsed {len(df)} records\")\n",
    "print(f\"Positive rate: {df['label'].mean():.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f39d47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate by text\n",
    "df = df.drop_duplicates(subset=[\"text\"]).reset_index(drop=True)\n",
    "print(f\"After deduplication: {len(df)} records\")\n",
    "\n",
    "# Stratified 8/1/1 split\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, temp_idx = next(sss1.split(df, df[\"label\"]))\n",
    "temp = df.iloc[temp_idx]\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "val_rel_idx, test_rel_idx = next(sss2.split(temp, temp[\"label\"]))\n",
    "val_idx  = temp_idx[val_rel_idx]\n",
    "test_idx = temp_idx[test_rel_idx]\n",
    "\n",
    "print(f\"Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eca7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export CSVs\n",
    "df.iloc[train_idx][[\"text\",\"label\"]].to_csv(os.path.join(OUT_DIR, \"hatexplain_train.csv\"), index=False)\n",
    "df.iloc[val_idx  ][[\"text\",\"label\"]].to_csv(os.path.join(OUT_DIR, \"hatexplain_val.csv\"  ), index=False)\n",
    "df.iloc[test_idx ][[\"text\",\"label\"]].to_csv(os.path.join(OUT_DIR, \"hatexplain_test.csv\" ), index=False)\n",
    "\n",
    "print(\"CSVs exported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53afefa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save protocols\n",
    "splits = {\n",
    "    \"hatexplain\": {\n",
    "        \"train_n\": int(len(train_idx)),\n",
    "        \"val_n\": int(len(val_idx)),\n",
    "        \"test_n\": int(len(test_idx)),\n",
    "        \"pos_rate\": {\n",
    "            \"train\": float(df.iloc[train_idx][\"label\"].mean()),\n",
    "            \"val\":   float(df.iloc[val_idx][\"label\"].mean()),\n",
    "            \"test\":  float(df.iloc[test_idx][\"label\"].mean()),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(OUT_DIR, \"hatexplain_protocols.json\"), \"w\") as f:\n",
    "    json.dump(splits, f, indent=2)\n",
    "\n",
    "print(\"HateXplain preprocessing complete!\")\n",
    "!ls -lh {OUT_DIR}/hatexplain*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac13ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
