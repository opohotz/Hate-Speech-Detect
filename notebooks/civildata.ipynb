{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6b76f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Cell - Add this at the top of each notebook\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Set base directories based on environment\n",
    "if IS_KAGGLE:\n",
    "    INPUT_ROOT = \"/kaggle/input\"\n",
    "    WORK_DIR = \"/kaggle/working\"\n",
    "elif IS_COLAB:\n",
    "    INPUT_ROOT = \"/content/input\"\n",
    "    WORK_DIR = \"/content/working\"\n",
    "else:\n",
    "    # Local environment\n",
    "    INPUT_ROOT = Path.cwd() / \"input\"\n",
    "    WORK_DIR = Path.cwd() / \"working\"\n",
    "\n",
    "# Create standard directories\n",
    "OUT_DIR = os.path.join(WORK_DIR, \"data\")\n",
    "EXPERIMENTS_DIR = os.path.join(WORK_DIR, \"experiments\")\n",
    "SCRIPTS_DIR = os.path.join(WORK_DIR, \"scripts\")\n",
    "\n",
    "# Create all directories\n",
    "for directory in [OUT_DIR, EXPERIMENTS_DIR, SCRIPTS_DIR]:\n",
    "    Path(directory).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"Environment: {'Kaggle' if IS_KAGGLE else 'Colab' if IS_COLAB else 'Local'}\")\n",
    "print(f\"Input directory: {INPUT_ROOT}\")\n",
    "print(f\"Working directory: {WORK_DIR}\")\n",
    "print(f\"Data directory: {OUT_DIR}\")\n",
    "print(f\"Experiments directory: {EXPERIMENTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d5d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Civil Comments Dataset Preprocessing\n",
    "import os, re, json\n",
    "import numpy as np, pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "INPUT_ROOT = \"/kaggle/input\"\n",
    "WORK_DIR   = \"/kaggle/working\"\n",
    "OUT_DIR    = os.path.join(WORK_DIR, \"data\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# Locate Civil Comments dataset\n",
    "# Adjust the directory name pattern based on your Kaggle dataset\n",
    "CIVIL_DIRS = [os.path.join(INPUT_ROOT, d) for d in os.listdir(INPUT_ROOT)\n",
    "              if \"civil\" in d.lower() and \"comment\" in d.lower()]\n",
    "\n",
    "if len(CIVIL_DIRS) == 0:\n",
    "    print(\"[WARN] Civil Comments dataset not found. Please add it to Kaggle inputs.\")\n",
    "    print(\"Expected pattern: directory containing 'civil' and 'comment' in name\")\n",
    "else:\n",
    "    CIVIL_DIR = CIVIL_DIRS[0]\n",
    "    print(f\"Found Civil Comments at: {CIVIL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identity columns in Civil Comments\n",
    "# These may vary - adjust based on your dataset\n",
    "IDENTITY_COLS_CIVIL = [\n",
    "    \"male\", \"female\", \"transgender\", \"other_gender\",\n",
    "    \"heterosexual\", \"homosexual_gay_or_lesbian\", \"bisexual\", \"other_sexual_orientation\",\n",
    "    \"christian\", \"jewish\", \"muslim\", \"hindu\", \"buddhist\", \"atheist\", \"other_religion\",\n",
    "    \"black\", \"white\", \"latino\", \"other_race_or_ethnicity\",\n",
    "    \"physical_disability\", \"intellectual_or_learning_disability\", \n",
    "    \"psychiatric_or_mental_illness\", \"other_disability\"\n",
    "]\n",
    "\n",
    "# Load Civil Comments - adjust filename as needed\n",
    "# Common filenames: \"all_data.csv\", \"civil_comments.csv\", \"train.csv\"\n",
    "civil_files = [f for f in os.listdir(CIVIL_DIR) if f.endswith(\".csv\")]\n",
    "if not civil_files:\n",
    "    raise FileNotFoundError(f\"No CSV files found in {CIVIL_DIR}\")\n",
    "\n",
    "civil_file = civil_files[0]  # Take first CSV\n",
    "print(f\"Loading: {civil_file}\")\n",
    "\n",
    "df_raw = pd.read_csv(os.path.join(CIVIL_DIR, civil_file))\n",
    "print(f\"Loaded {len(df_raw)} records\")\n",
    "print(f\"Columns: {list(df_raw.columns[:10])}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning function\n",
    "URL_RE = re.compile(r\"http\\S+\")\n",
    "AT_RE  = re.compile(r\"@\\w+\")\n",
    "\n",
    "def clean_text(s: str) -> str:\n",
    "    s = str(s) if pd.notna(s) else \"\"\n",
    "    s = URL_RE.sub(\" URL \", s)\n",
    "    s = AT_RE.sub(\"@USER\", s)\n",
    "    s = s.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Create cleaned dataset\n",
    "# Adjust column names based on your dataset structure\n",
    "text_col = \"comment_text\" if \"comment_text\" in df_raw.columns else \"text\"\n",
    "toxicity_col = \"toxicity\" if \"toxicity\" in df_raw.columns else \"target\"\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"id\": df_raw.get(\"id\", range(len(df_raw))),\n",
    "    \"text\": df_raw[text_col].map(clean_text),\n",
    "    \"label\": (df_raw[toxicity_col] >= 0.5).astype(int)\n",
    "})\n",
    "\n",
    "# Add identity group columns\n",
    "use_id_cols = [c for c in IDENTITY_COLS_CIVIL if c in df_raw.columns]\n",
    "for c in use_id_cols:\n",
    "    df[f\"g_{c}\"] = (df_raw[c].fillna(0) >= 0.5).astype(int)\n",
    "\n",
    "print(f\"Created dataset with {len(df)} records\")\n",
    "print(f\"Identity columns found: {len(use_id_cols)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate by text\n",
    "df = df.drop_duplicates(subset=[\"text\"]).reset_index(drop=True)\n",
    "print(f\"After deduplication: {len(df)} records\")\n",
    "\n",
    "# Stratified 8/1/1 split\n",
    "sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "train_idx, temp_idx = next(sss1.split(df, df[\"label\"]))\n",
    "temp = df.iloc[temp_idx]\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "val_rel_idx, test_rel_idx = next(sss2.split(temp, temp[\"label\"]))\n",
    "val_idx  = temp_idx[val_rel_idx]\n",
    "test_idx = temp_idx[test_rel_idx]\n",
    "\n",
    "print(f\"Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a2bb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export standard CSVs (text, label)\n",
    "df.iloc[train_idx][[\"text\",\"label\"]].to_csv(os.path.join(OUT_DIR, \"civil_train.csv\"), index=False)\n",
    "df.iloc[val_idx  ][[\"text\",\"label\"]].to_csv(os.path.join(OUT_DIR, \"civil_val.csv\"  ), index=False)\n",
    "df.iloc[test_idx ][[\"text\",\"label\"]].to_csv(os.path.join(OUT_DIR, \"civil_test.csv\" ), index=False)\n",
    "\n",
    "print(\"Standard CSVs exported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11edc788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export full CSVs (with id and group attributes)\n",
    "group_cols = [\"id\", \"text\", \"label\"] + [f\"g_{c}\" for c in use_id_cols]\n",
    "df.iloc[train_idx][group_cols].to_csv(os.path.join(OUT_DIR, \"civil_train_full.csv\"), index=False)\n",
    "df.iloc[val_idx  ][group_cols].to_csv(os.path.join(OUT_DIR, \"civil_val_full.csv\"  ), index=False)\n",
    "df.iloc[test_idx ][group_cols].to_csv(os.path.join(OUT_DIR, \"civil_test_full.csv\" ), index=False)\n",
    "\n",
    "print(\"Full CSVs (with group attributes) exported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd29032d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save protocols\n",
    "splits = {\n",
    "    \"civil\": {\n",
    "        \"train_n\": int(len(train_idx)),\n",
    "        \"val_n\": int(len(val_idx)),\n",
    "        \"test_n\": int(len(test_idx)),\n",
    "        \"pos_rate\": {\n",
    "            \"train\": float(df.iloc[train_idx][\"label\"].mean()),\n",
    "            \"val\":   float(df.iloc[val_idx][\"label\"].mean()),\n",
    "            \"test\":  float(df.iloc[test_idx][\"label\"].mean()),\n",
    "        }\n",
    "    }\n",
    "}\n",
    "with open(os.path.join(OUT_DIR, \"civil_protocols.json\"), \"w\") as f:\n",
    "    json.dump(splits, f, indent=2)\n",
    "\n",
    "print(\"Civil Comments preprocessing complete!\")\n",
    "!ls -lh {OUT_DIR}/civil*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a5329f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b21eb3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
