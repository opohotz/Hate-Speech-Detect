{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd2d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration Cell - Environment Detection\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Detect environment\n",
    "IS_KAGGLE = os.path.exists('/kaggle/input')\n",
    "IS_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "# Set base directories based on environment\n",
    "if IS_KAGGLE:\n",
    "    PROJECT_ROOT = Path(\"/kaggle/working\")\n",
    "    DATA_DIR = Path(\"/kaggle/input/ood-eval-toxic-classifiers/data\")\n",
    "elif IS_COLAB:\n",
    "    PROJECT_ROOT = Path(\"/content\")\n",
    "    DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "else:\n",
    "    # Local environment - use parent of notebooks folder\n",
    "    PROJECT_ROOT = Path(__file__).parent.parent if '__file__' in dir() else Path.cwd().parent\n",
    "    if not (PROJECT_ROOT / \"data\").exists():\n",
    "        PROJECT_ROOT = Path.cwd().parent  # Fallback\n",
    "    DATA_DIR = PROJECT_ROOT / \"data\"\n",
    "\n",
    "# Standard directories\n",
    "EXPERIMENTS_DIR = PROJECT_ROOT / \"experiments\"\n",
    "SCRIPTS_DIR = PROJECT_ROOT / \"scripts\"\n",
    "\n",
    "# Create directories if needed\n",
    "EXPERIMENTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(EXPERIMENTS_DIR / \"plots\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Add scripts to path\n",
    "if str(SCRIPTS_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SCRIPTS_DIR))\n",
    "\n",
    "print(f\"Environment: {'Kaggle' if IS_KAGGLE else 'Colab' if IS_COLAB else 'Local'}\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Experiments directory: {EXPERIMENTS_DIR}\")\n",
    "print(f\"Scripts directory: {SCRIPTS_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b4bb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Master Experiment Runner for CS483 BiasBreakers Project\n",
    "# This notebook orchestrates all experiments and generates results for the final report\n",
    "\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BiasBreakers: Master Experiment Runner\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nData directory: {DATA_DIR}\")\n",
    "print(f\"Experiments directory: {EXPERIMENTS_DIR}\")\n",
    "print(f\"Scripts directory: {SCRIPTS_DIR}\")\n",
    "\n",
    "# Verify directories exist\n",
    "if not DATA_DIR.exists():\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: Data directory not found: {DATA_DIR}\")\n",
    "else:\n",
    "    print(f\"\\n‚úì Data directory exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c33a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Verify data preprocessing is complete\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 1: Verifying Data Files\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "required_files = [\n",
    "    \"jigsaw_train.csv\", \"jigsaw_val.csv\", \"jigsaw_test.csv\",\n",
    "    \"jigsaw_train_full.csv\", \"jigsaw_val_full.csv\", \"jigsaw_test_full.csv\",\n",
    "    \"civil_train.csv\", \"civil_val.csv\", \"civil_test.csv\",\n",
    "    \"civil_train_full.csv\", \"civil_val_full.csv\", \"civil_test_full.csv\",\n",
    "    \"hatexplain_train.csv\", \"hatexplain_val.csv\", \"hatexplain_test.csv\",\n",
    "]\n",
    "\n",
    "missing_files = []\n",
    "found_files = []\n",
    "for f in required_files:\n",
    "    path = DATA_DIR / f\n",
    "    if path.exists():\n",
    "        size_mb = path.stat().st_size / (1024*1024)\n",
    "        print(f\"‚úì {f} ({size_mb:.1f} MB)\")\n",
    "        found_files.append(f)\n",
    "    else:\n",
    "        print(f\"‚úó {f} - MISSING\")\n",
    "        missing_files.append(f)\n",
    "\n",
    "if missing_files:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: {len(missing_files)} files missing!\")\n",
    "    print(\"Please run preprocessing scripts first:\")\n",
    "    print(\"  python scripts/process_raw_data.py\")\n",
    "else:\n",
    "    print(f\"\\n‚úì All {len(required_files)} data files found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Run TF-IDF Baselines\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 2: Training TF-IDF Baselines\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Check if script exists\n",
    "tfidf_script = SCRIPTS_DIR / \"run_tfidf_baselines.py\"\n",
    "if not tfidf_script.exists():\n",
    "    print(f\"‚ö†Ô∏è  ERROR: Script not found: {tfidf_script}\")\n",
    "    raise FileNotFoundError(f\"Missing script: {tfidf_script}\")\n",
    "\n",
    "print(f\"‚úì Found TF-IDF script: {tfidf_script}\")\n",
    "\n",
    "# Try to import and run directly\n",
    "try:\n",
    "    from run_tfidf_baselines import train_and_evaluate_tfidf\n",
    "    \n",
    "    # Experiment 2.1: Jigsaw ‚Üí Civil & HateXplain\n",
    "    print(\"\\nRunning: TF-IDF Logistic Regression (Jigsaw ‚Üí Civil, HateXplain)\")\n",
    "    results_tfidf = train_and_evaluate_tfidf(\n",
    "        source_dataset=\"jigsaw\",\n",
    "        target_datasets=[\"civil\", \"hatexplain\"],\n",
    "        model_type=\"logreg\",\n",
    "        seed=42,\n",
    "        data_dir=str(DATA_DIR),\n",
    "        save_preds=True,\n",
    "    )\n",
    "    \n",
    "    print(\"\\n‚úì TF-IDF baseline complete!\")\n",
    "    if results_tfidf and 'in_domain_test' in results_tfidf:\n",
    "        print(f\"In-domain test F1: {results_tfidf['in_domain_test']['f1']:.4f}\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Import Error: {e}\")\n",
    "    print(\"Running as subprocess instead...\")\n",
    "    \n",
    "    # Fallback: Run as subprocess\n",
    "    result = subprocess.run([\n",
    "        sys.executable,\n",
    "        str(tfidf_script),\n",
    "        \"--source_dataset\", \"jigsaw\",\n",
    "        \"--target_datasets\", \"civil\", \"hatexplain\",\n",
    "        \"--model\", \"logreg\",\n",
    "        \"--seed\", \"42\",\n",
    "        \"--data_dir\", str(DATA_DIR),\n",
    "        \"--save_preds\"\n",
    "    ], capture_output=True, text=True, cwd=str(PROJECT_ROOT))\n",
    "    \n",
    "    print(result.stdout)\n",
    "    if result.returncode != 0:\n",
    "        print(\"STDERR:\", result.stderr)\n",
    "    results_tfidf = None\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error running TF-IDF: {e}\")\n",
    "    results_tfidf = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a185d969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Run RoBERTa Models (Optional - requires GPU)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 3: RoBERTa Models (GPU Required)\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "import torch\n",
    "\n",
    "# Check GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    try:\n",
    "        from run_roberta import train_and_evaluate\n",
    "        \n",
    "        # Experiment 3.1: Basic RoBERTa with calibration\n",
    "        print(\"\\n--- Experiment 3.1: RoBERTa with Isotonic Calibration ---\")\n",
    "        results_roberta = train_and_evaluate(\n",
    "            source_dataset=\"jigsaw\",\n",
    "            target_datasets=[\"civil\", \"hatexplain\"],\n",
    "            model_name=\"roberta-base\",\n",
    "            epochs=3,\n",
    "            batch_size=16,\n",
    "            lr=2e-5,\n",
    "            max_len=128,\n",
    "            seed=42,\n",
    "            data_dir=str(DATA_DIR),\n",
    "            calibration=\"isotonic\",\n",
    "            early_stop=True,\n",
    "            patience=2,\n",
    "            tune_threshold=True,\n",
    "            save_preds=True,\n",
    "        )\n",
    "        \n",
    "        print(\"\\n‚úì RoBERTa training complete!\")\n",
    "        if results_roberta and 'in_domain' in results_roberta:\n",
    "            print(f\"In-domain test F1: {results_roberta['in_domain']['test']['f1']:.4f}\")\n",
    "    except ImportError as e:\n",
    "        print(f\"‚ö†Ô∏è  Could not import RoBERTa script: {e}\")\n",
    "        results_roberta = None\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Error running RoBERTa: {e}\")\n",
    "        results_roberta = None\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  No GPU available. Skipping RoBERTa training.\")\n",
    "    print(\"To run RoBERTa models, use Kaggle or Google Colab with GPU enabled.\")\n",
    "    results_roberta = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b72725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Compute Fairness Metrics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 4: Computing Fairness Metrics\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "try:\n",
    "    from fairness_metrics import compute_group_fairness\n",
    "    \n",
    "    # Experiment 4.1: Cross-domain fairness (Jigsaw test set)\n",
    "    print(\"Computing fairness for: Jigsaw test set\")\n",
    "    \n",
    "    # Load predictions and full data\n",
    "    pred_files = list(EXPERIMENTS_DIR.glob(\"preds_*.csv\"))\n",
    "    print(f\"Found {len(pred_files)} prediction files\")\n",
    "    \n",
    "    for pred_file in pred_files:\n",
    "        print(f\"\\nProcessing: {pred_file.name}\")\n",
    "        \n",
    "        # Determine which full data file to use\n",
    "        if \"jigsaw_test\" in pred_file.name:\n",
    "            full_data_file = DATA_DIR / \"jigsaw_test_full.csv\"\n",
    "        elif \"civil\" in pred_file.name:\n",
    "            full_data_file = DATA_DIR / \"civil_test_full.csv\"\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        if not full_data_file.exists():\n",
    "            print(f\"  ‚ö†Ô∏è  Full data file not found: {full_data_file}\")\n",
    "            continue\n",
    "            \n",
    "        pred_df = pd.read_csv(pred_file)\n",
    "        full_df = pd.read_csv(full_data_file)\n",
    "        \n",
    "        # Merge on ID if both have id columns\n",
    "        if 'id' in pred_df.columns and 'id' in full_df.columns:\n",
    "            merged_df = pred_df.merge(full_df, on=\"id\", how=\"inner\", suffixes=(\"\", \"_full\"))\n",
    "            if \"label_full\" in merged_df.columns:\n",
    "                merged_df = merged_df.drop(columns=[\"label_full\"])\n",
    "        else:\n",
    "            # Assume same order\n",
    "            merged_df = pd.concat([pred_df, full_df.drop(columns=['text', 'label'], errors='ignore')], axis=1)\n",
    "        \n",
    "        # Find group columns\n",
    "        group_cols = [c for c in merged_df.columns if c.startswith(\"g_\")]\n",
    "        \n",
    "        if len(group_cols) == 0:\n",
    "            print(f\"  ‚ö†Ô∏è  No group columns found\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"  Found {len(group_cols)} identity groups\")\n",
    "        print(f\"  Analyzing {len(merged_df)} predictions\")\n",
    "        \n",
    "        # Compute fairness\n",
    "        summary_df, per_group_df = compute_group_fairness(\n",
    "            merged_df,\n",
    "            group_cols=group_cols,\n",
    "            label_col=\"label\",\n",
    "            pred_col=\"pred\",\n",
    "        )\n",
    "        \n",
    "        # Save results\n",
    "        output_name = pred_file.stem.replace(\"preds_\", \"fairness_\")\n",
    "        summary_df.to_csv(EXPERIMENTS_DIR / f\"{output_name}_summary.csv\", index=False)\n",
    "        per_group_df.to_csv(EXPERIMENTS_DIR / f\"{output_name}_per_group.csv\", index=False)\n",
    "        \n",
    "        # Print top fairness violations\n",
    "        print(f\"\\n  Top 3 groups by Demographic Parity difference:\")\n",
    "        print(summary_df.nlargest(3, \"dp_diff\")[[\"group_col\", \"dp_diff\"]].to_string(index=False))\n",
    "        \n",
    "    print(\"\\n‚úì Fairness analysis complete!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not import fairness_metrics: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Error computing fairness: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82766dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: Generate Summary Statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 5: Summary Statistics\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Load all summary CSVs\n",
    "summary_files = list(EXPERIMENTS_DIR.glob(\"summary_*.csv\"))\n",
    "print(f\"Found {len(summary_files)} summary files:\\n\")\n",
    "\n",
    "all_summaries = {}\n",
    "for f in summary_files:\n",
    "    df = pd.read_csv(f)\n",
    "    model_name = f.stem.replace(\"summary_\", \"\")\n",
    "    all_summaries[model_name] = df\n",
    "    \n",
    "    print(f\"--- {model_name} ---\")\n",
    "    cols_to_show = [c for c in [\"split\", \"accuracy\", \"f1\", \"auroc\", \"pr_auc\"] if c in df.columns]\n",
    "    print(df[cols_to_show].to_string(index=False))\n",
    "    print()\n",
    "\n",
    "if all_summaries:\n",
    "    # Create comparison table\n",
    "    comparison_rows = []\n",
    "    for model_name, df in all_summaries.items():\n",
    "        for _, row in df.iterrows():\n",
    "            comparison_rows.append({\n",
    "                \"Model\": model_name,\n",
    "                \"Split\": row.get(\"split\", \"unknown\"),\n",
    "                \"Accuracy\": f\"{row.get('accuracy', 0):.4f}\",\n",
    "                \"F1\": f\"{row.get('f1', 0):.4f}\",\n",
    "                \"AUROC\": f\"{row.get('auroc', 0):.4f}\",\n",
    "                \"PR-AUC\": f\"{row.get('pr_auc', 0):.4f}\",\n",
    "            })\n",
    "\n",
    "    comparison_df = pd.DataFrame(comparison_rows)\n",
    "    comparison_df.to_csv(EXPERIMENTS_DIR / \"model_comparison.csv\", index=False)\n",
    "    print(\"‚úì Model comparison saved to: model_comparison.csv\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No summary files found. Run experiments first.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623503f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 6: Quick Visualization Preview\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STEP 6: Quick Visualization Preview\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "if all_summaries:\n",
    "    # Plot 1: Cross-domain performance comparison\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "    models_to_plot = list(all_summaries.keys())[:4]  # Plot up to 4 models\n",
    "    colors = plt.cm.Set2(range(len(models_to_plot)))\n",
    "\n",
    "    bar_width = 0.8 / max(len(models_to_plot), 1)\n",
    "    \n",
    "    for idx, model_name in enumerate(models_to_plot):\n",
    "        df = all_summaries[model_name]\n",
    "        splits = df[\"split\"].values\n",
    "        f1_scores = df[\"f1\"].values\n",
    "        \n",
    "        x_pos = np.arange(len(splits)) + idx * bar_width\n",
    "        ax.bar(x_pos, f1_scores, width=bar_width, label=model_name, alpha=0.8)\n",
    "\n",
    "    ax.set_xlabel(\"Dataset Split\", fontsize=12)\n",
    "    ax.set_ylabel(\"F1 Score\", fontsize=12)\n",
    "    ax.set_title(\"Cross-Domain Performance Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "    \n",
    "    # Set x-ticks\n",
    "    if len(models_to_plot) > 0 and len(all_summaries[models_to_plot[0]]) > 0:\n",
    "        splits = all_summaries[models_to_plot[0]][\"split\"].values\n",
    "        ax.set_xticks(np.arange(len(splits)) + bar_width * (len(models_to_plot) - 1) / 2)\n",
    "        ax.set_xticklabels(splits, rotation=45, ha=\"right\")\n",
    "    \n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3, axis=\"y\")\n",
    "    ax.set_ylim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Ensure plots directory exists\n",
    "    plots_dir = EXPERIMENTS_DIR / \"plots\"\n",
    "    plots_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    plt.savefig(plots_dir / \"quick_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n‚úì Quick visualization complete!\")\n",
    "    print(\"For full analysis, run: notebooks/analysis_plots.ipynb\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No summary data available for visualization.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3d07f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FINAL: Experiment Summary Report\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT SUMMARY REPORT\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "print(\"üìä COMPLETED EXPERIMENTS:\")\n",
    "print(\"  ‚úì TF-IDF Baseline (Logistic Regression)\" if results_tfidf else \"  ‚ö†Ô∏è TF-IDF Baseline (skipped or failed)\")\n",
    "print(\"  ‚úì RoBERTa with Calibration\" if results_roberta else \"  ‚ö†Ô∏è RoBERTa (skipped - no GPU)\")\n",
    "print(\"  ‚úì Cross-Domain Evaluation\")\n",
    "print(\"  ‚úì Fairness Analysis\")\n",
    "print(\"  ‚úì Summary Statistics\\n\")\n",
    "\n",
    "print(\"üìÅ OUTPUT FILES:\")\n",
    "output_files = list(EXPERIMENTS_DIR.glob(\"*\"))\n",
    "print(f\"  Total files generated: {len(output_files)}\")\n",
    "print(f\"  Summary CSVs: {len(list(EXPERIMENTS_DIR.glob('summary_*.csv')))}\")\n",
    "print(f\"  Prediction CSVs: {len(list(EXPERIMENTS_DIR.glob('preds_*.csv')))}\")\n",
    "print(f\"  Fairness CSVs: {len(list(EXPERIMENTS_DIR.glob('fairness_*.csv')))}\")\n",
    "\n",
    "print(\"\\nüìà NEXT STEPS:\")\n",
    "print(\"  1. Run notebooks/analysis_plots.ipynb to generate all visualizations\")\n",
    "print(\"  2. Review fairness metrics in fairness_*_summary.csv\")\n",
    "print(\"  3. Copy key plots from experiments/plots/ to your report\")\n",
    "print(\"  4. Use model_comparison.csv for quantitative results table\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Experiment run complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b259be9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
